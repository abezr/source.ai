---
description: Run the RAG evaluation harness.
agent: qa
---

Execute the evaluation script at `scripts/run_evaluation.py`. Report the key metrics (e.g., Faithfulness, Answer Relevancy). If any metric is below the required threshold, analyze the root cause and suggest a fix.