{"question": "What is the main purpose of the Hybrid Book Index system?", "ground_truth": "The Hybrid Book Index (HBI) system is designed to ingest PDF/DjVu books, extract their structure including Table of Contents, and provide a high-quality Retrieval-Augmented Generation (RAG) API for querying book content with anti-hallucination safeguards.", "contexts": ["The Hybrid Book Index (HBI) System is a production-grade RAG system built by AI agents, featuring full observability and SDLC quality gates. The system ingests PDF/DjVu books, extracts their structure, performs hybrid RAG with lazy chapter expansion, and exposes a governed API & Ops Portal.", "Our mission is to build an enterprise-grade, observable, and governed system for indexing and querying PDF/DjVu books. The system must support lazy loading of content on demand and provide a high-quality RAG API."]}
{"question": "How does the system prevent hallucinations in generated answers?", "ground_truth": "The system prevents hallucinations through multiple layers: mandatory citations requiring every claim to reference specific source chunks, confidence scoring where answers below a threshold are rejected, retrieval gates that ensure sufficient relevant context is found before generation, and structured output validation that enforces proper citation formatting.", "contexts": ["Every claim you make MUST be supported by a specific context passage. For each claim made in the answer, provide the source_chunk_id and page_number from the context passage that supports it. Provide a confidence_score from 0.0 to 1.0 indicating your certainty that the answer is fully supported by the context.", "The system implements anti-hallucination gates: retrieval gates ensure sufficient relevant context is found, generation gates validate confidence scores, and fallback messages are provided when quality gates prevent answering.", "If the confidence_score is below a threshold (e.g., 0.7), or if the claims list is empty, the system abstains from answering and returns a helpful fallback message."]}
{"question": "What are the key components of the system's architecture?", "ground_truth": "The system architecture includes: API & Orchestrator (FastAPI application with LangGraph), Agents Worker Pool (Parser, Indexer, etc.), Storage (SQLite with FTS5, sqlite-vec, MinIO), Observability (Langfuse/Phoenix, Grafana/Prometheus/Loki), and CI/CD quality gates with RAGAS evaluation.", "contexts": ["The system architecture includes: API & Orchestrator (FastAPI, LangGraph), Agents Worker Pool (Ingestor, Structure, Index Parser, Chunker & Embedder, RAG Router, Answerer, Evaluator), Text & Lexical Index (SQLite FTS5), Vector Index (sqlite-vec), Object Storage (MinIO), and comprehensive observability stack.", "Key architectural components: API & Orchestrator handles HTTP requests and orchestrates agent workflows, Workers Pool executes specialized agent tasks, Storage includes SQLite with FTS5 for text search and sqlite-vec for vector search, and full observability with Langfuse for LLM traces and Grafana stack for system telemetry."]}
{"question": "What evaluation metrics does the system use to ensure quality?", "ground_truth": "The system uses RAGAS evaluation framework with key metrics including faithfulness (answer grounded in context), answer relevancy (answer addresses the question), context precision (retrieved context is relevant), and context recall (all relevant information retrieved).", "contexts": ["The system integrates RAGAS evaluation framework to measure faithfulness, answer_relevancy, context_precision, and context_recall on every code change. This creates an automated quality gate that prevents regressions in answer reliability.", "RAGAS metrics include: faithfulness (does the answer contradict or hallucinate information not in context?), answer_relevancy (is the answer relevant to the question?), context_precision (is the retrieved context relevant to the answer?), and context_recall (does the retrieved context contain all necessary information?).", "The evaluation harness runs RAGAS metrics against a golden dataset and fails the build if metrics fall below predefined thresholds, ensuring the system maintains high quality on every change."]}
{"question": "How does the hybrid retrieval mechanism work?", "ground_truth": "The hybrid retrieval combines lexical search (keyword matching using SQLite FTS5 with BM25 scoring) and semantic search (vector similarity using sqlite-vec) using Reciprocal Rank Fusion (RRF) to provide more robust and relevant results than either method alone.", "contexts": ["The retrieval function combines lexical (keyword) search and semantic (vector) search using Reciprocal Rank Fusion (RRF) algorithm to provide robust ranking from both keyword matches and semantic meaning.", "Lexical search uses SQLite FTS5 with BM25 scoring to find chunks matching query keywords, while vector search uses sqlite-vec to find semantically similar chunks based on embedding similarity.", "Reciprocal Rank Fusion combines the two ranked lists by calculating RRF scores for each chunk, providing a more robust ranking that benefits from both exact keyword matches and semantic understanding."]}